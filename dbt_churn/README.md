# ‚òÅÔ∏è Cloud Data Engineering ‚Äî End-to-End ETL Pipeline
**Python ‚Ä¢ Apache Airflow ‚Ä¢ dbt ‚Ä¢ Snowflake ‚Ä¢ PostgreSQL ‚Ä¢ Docker ‚Ä¢ Power BI ‚Ä¢ Git**

Este reposit√≥rio apresenta a constru√ß√£o de um **pipeline de dados end-to-end em ambiente cloud**, cobrindo **ingest√£o, orquestra√ß√£o, transforma√ß√£o, modelagem anal√≠tica e qualidade de dados**, seguindo boas pr√°ticas modernas de **Data Engineering e Analytics Engineering**.

O projeto foi desenhado para simular um **cen√°rio real de produ√ß√£o**, indo al√©m de solu√ß√µes totalmente gerenciadas, com foco em **infraestrutura, automa√ß√£o, versionamento e observabilidade**.

---

## üéØ Objetivos do Projeto

- Construir um pipeline **completo**, desde dados brutos at√© camadas anal√≠ticas.
- Trabalhar com **dados externos** (API p√∫blica e dataset real).
- Implementar **orquestra√ß√£o automatizada** com depend√™ncias, retries e logs.
- Aplicar **Medallion Architecture (Bronze, Silver, Gold)** usando dbt.
- Garantir **qualidade de dados** via testes automatizados.
- Simular um ambiente pr√≥ximo ao **dia a dia de um Data Engineer em cloud**.

---

## üß± Stack Tecnol√≥gica

### ‚òÅÔ∏è Infraestrutura & DevOps
- **Cloud Provider:** DigitalOcean
- **Ambiente:** Linux VM (Droplet)
- **Containeriza√ß√£o:** Docker & Docker Compose
- **CI:** GitHub Actions

> Escolha proposital para aprender conceitos reais de infraestrutura, deploy e isolamento de ambientes.

---

### üóÑÔ∏è Camada de Dados
- **PostgreSQL**
- **Snowflake** (Data Warehouse Anal√≠tico)

Utiliza√ß√£o:
- **Bronze:** dados brutos ingeridos
- **Silver:** dados tratados e padronizados
- **Gold:** dados modelados para analytics e BI

---

### üîå Ingest√£o de Dados
- **Python**
  - `requests`
  - `pandas`
  - `SQLAlchemy`

**Responsabilidades:**
- Extra√ß√£o de dados (API p√∫blica / CSV real)
- Padroniza√ß√£o t√©cnica m√≠nima de schema
- Persist√™ncia **sem regras de neg√≥cio** na camada Bronze

---

### üîÑ Orquestra√ß√£o
- **Apache Airflow**

**Funcionalidades implementadas:**
- DAGs end-to-end
- Controle de depend√™ncias
- Retries autom√°ticos
- Scheduling e backfill
- Logs, Grid View e Gantt View para observabilidade

---

### üß™ Transforma√ß√£o & Modelagem
- **dbt Core**

**Boas pr√°ticas aplicadas:**
- SQL versionado
- Modelagem incremental
- Medallion Architecture
- Separa√ß√£o entre l√≥gica t√©cnica e de neg√≥cio

**Camadas:**
- **Bronze:** espelhamento do raw
- **Silver:** limpeza, tipagem, deduplica√ß√£o
- **Gold:** m√©tricas e tabelas anal√≠ticas

**Qualidade de Dados:**
- `not_null`
- `unique`
- testes customizados via dbt

---

### üìä Consumo Anal√≠tico
- **Power BI**

**KPIs e An√°lises:**
- Taxa de Churn (%)
- Segmenta√ß√£o geogr√°fica
- Comportamento do cliente
- Uso de produtos e impacto na reten√ß√£o

---

## üìÖ Linha do Tempo do Projeto

### üèóÔ∏è Fase 1 ‚Äî Funda√ß√£o e Infraestrutura
- Configura√ß√£o do `docker-compose.yaml`
- Deploy do Apache Airflow
- Cria√ß√£o dos schemas `BRONZE`, `SILVER`, `GOLD`

---

### üì• Fase 2 ‚Äî Ingest√£o & Orquestra√ß√£o
- Desenvolvimento do script Python de ingest√£o
- Padroniza√ß√£o t√©cnica de colunas
- Carga autom√°tica na camada Bronze
- Cria√ß√£o da primeira DAG no Airflow

---

### üß† Fase 3 ‚Äî Analytics Engineering
- Modelos dbt para Silver e Gold
- Implementa√ß√£o de regras de neg√≥cio
- Testes automatizados de qualidade
- Orquestra√ß√£o do `dbt run` via Airflow

---

### üìà Fase 4 ‚Äî Entrega de Valor
- Conex√£o do Power BI √† camada Gold
- Constru√ß√£o de dashboards estrat√©gicos
- Gera√ß√£o de insights para reten√ß√£o de clientes

---

## üè¶ Caso de Uso: Churn Banc√°rio

Pipeline focado em **reten√ß√£o de clientes banc√°rios**, transformando dados operacionais em intelig√™ncia estrat√©gica.

**Principais an√°lises:**
- Taxa geral de churn
- Churn por pa√≠s (Fran√ßa, Alemanha, Espanha)
- Rela√ß√£o entre produtos, atividade do cliente e churn

---

## üõ†Ô∏è Skills Aplicadas

| Skill | Categoria |
|-----|---------|
| Apache Airflow | Orquestra√ß√£o |
| dbt (data build tool) | Transforma√ß√£o |
| SQL (PostgreSQL / Snowflake) | Data Warehouse |
| Python (ETL) | Engenharia de Dados |
| Docker & Docker Compose | Infraestrutura |
| Power BI | Visualiza√ß√£o |
| Git & GitHub Actions | Versionamento & CI |

---

## üöÄ Como Executar o Projeto

```bash
# Subir o ambiente
docker-compose up -d
Acessar o Airflow em http://localhost:8080

Ativar a DAG pipeline_churn_bancario_end_to_end

Aguardar a execu√ß√£o completa

Consultar os dados nas camadas Silver e Gold

Abrir o arquivo .pbix no Power BI para visualizar os dashboards
```

## üöÄ Como Executar o Projeto

### Pr√©-requisitos
- Docker e Docker Compose instalados
- Power BI Desktop (para visualiza√ß√£o do dashboard)

### Passo a Passo

1. Subir o ambiente local (Airflow, banco e depend√™ncias):
   ```bash
   docker-compose up -d
   ```
2. Acessar a interface do Airflow:

http://localhost:8080
> Credenciais padr√£o configuradas no docker-compose.yml

3. Ativar a DAG:
pipeline_churn_bancario_end_to_end

4. Aguardar a execu√ß√£o completa do pipeline.

5. Consultar os dados transformados nas camadas Silver e Gold.

6. Abrir o arquivo .pbix no Power BI para visualizar os dashboards.
